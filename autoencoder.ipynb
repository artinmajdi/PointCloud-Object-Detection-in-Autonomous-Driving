{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import open3d\n",
    "import pptk\n",
    "from logging import raiseExceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, modality = 'Radar', data_type='vector', normalize=True , dataset_directory='/home/artin/Documents/10dB_CR_10deg_in_2021-10-15-14-15-49'):\n",
    "\n",
    "        self.dataset_directory = dataset_directory\n",
    "        self.modality = modality\n",
    "        self.data_type = data_type\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if modality == 'Radar': \n",
    "            self.columns = ['Frame ID', 'Pocket ID' , 'Y', '-X', 'Z', 'Range', 'Azimuth Angle', 'Elevation Angle', 'RCS' , 'Velocity' , 'Range Index', 'Azimuth Index', 'Elevation Index', 'Velocity Index', 'Amplitude Index', 'Timestamp', 'Temperature', '?']\n",
    "            self.rename_columns_dict = {'Range Index':'Range', 'Azimuth Index':'Azimuth', 'Elevation Index':'Elevation', 'Amplitude Index':'Amplitude', 'RCS':'RCS', 'Velocity Index':'Velocity'}\n",
    "            \n",
    "        elif modality == 'Lidar':\n",
    "            self.columns = ['X', 'Y', 'Z', '?a' , '?b' , '?c']\n",
    "            self.rename_columns_dict =  {'X':'X', 'Y':'Y', 'Z':'Z', '?a':'?a', '?b':'?b' , '?c':'?c'}\n",
    "            \n",
    "            \n",
    "    def get_dataframe(self, dir: str):\n",
    "        \n",
    "        def _read_data(dir):\n",
    "            self.dataframe_original = pd.read_csv(dir)        \n",
    "            self.dataframe_original.columns = self.columns\n",
    "            self.dataframe = self.dataframe_original.copy()\n",
    "            \n",
    "        def _filter_columns(columns: list):\n",
    "            self.dataframe = self.dataframe[columns]\n",
    "        \n",
    "        def _rename_columns(columns: dict):\n",
    "            self.dataframe.rename(columns=columns, inplace=True)\n",
    "        \n",
    "        _read_data(dir=dir)\n",
    "        _filter_columns( columns=self.rename_columns_dict.keys() )\n",
    "        _rename_columns( columns=self.rename_columns_dict )\n",
    "        \n",
    "    def conversion_to_3D( self , matrix_dimentions={'Range':600, 'Azimuth':180, 'Elevation':20}, matrix_value='Amplitude'):\n",
    "        \n",
    "        ''' Range is assumed to be in the range [0, 600]\n",
    "            Elevation Angle is in the range [-10, 10]\n",
    "            Azimuth angle is in the range [0, 180]\n",
    "            matrix_value can either be an integer/float or a string that corresponds to a column name. It's value will be used to fill the matrix. '''\n",
    "        \n",
    "        \n",
    "        # Creating an empty array to store the data        \n",
    "        self.data_3d = np.zeros( list(matrix_dimentions.values()) )\n",
    "        \n",
    "        # Shifting the Elevation values to the range (0, 20)\n",
    "        self.dataframe.Elevation += 10\n",
    "        \n",
    "                \n",
    "        x,y,z = matrix_dimentions.keys()\n",
    "        \n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            self.data_3d [ int(row[x]) ] [ int(row[y]) ] [ int(row[z]) ] = row[matrix_value] if isinstance(matrix_value,str) else matrix_value\n",
    "            \n",
    "                    \n",
    "        return self.data_3d\n",
    "     \n",
    "    def normalizing_to_0_1( self, normalization_values = {'Range':1000, 'Azimuth':180, 'Elevation':20, 'RCS':100, 'Velocity':200, 'Amplitude':100000} ):\n",
    "\n",
    "        \n",
    "        for key, value in normalization_values.items():\n",
    "            self.dataframe[key] /= value\n",
    "        \n",
    "        self.dataframe.Elevation += 0.5\n",
    "        self.dataframe.Velocity  += 0.5\n",
    "    \n",
    "        return self.dataframe\n",
    "    \n",
    "    def get_data(self, filename = '1_.txt'):\n",
    "        ''' data_type can be 'vector' or 'matrix' '''\n",
    "        \n",
    "        dir = f'{self.dataset_directory}/{self.modality}/{filename}'\n",
    "        \n",
    "        self.get_dataframe(dir=dir)\n",
    "\n",
    "        if self.modality == 'Radar':\n",
    "            \n",
    "            if self.data_type == 'vector':   return self.normalizing_to_0_1() if self.normalize else self.dataframe\n",
    "            \n",
    "            elif self.data_type == 'matrix': return self.conversion_to_3D( matrix_dimentions={'Range':600, 'Azimuth':180, 'Elevation':20}, matrix_value=1)\n",
    "            \n",
    "            else: raise ValueError('data_type can be either \"vector\" or \"matrix\"')\n",
    "            \n",
    "        elif self.modality == 'Lidar':\n",
    "            return self.dataframe\n",
    "            \n",
    "\n",
    "    # @staticmethod\n",
    "    # def toSpherical(df):\n",
    "\n",
    "    #     theta = np.arctan2(np.sqrt(df['-X']**2 + df['Y']**2), df['Z'])\n",
    "\n",
    "    #     phi = np.arctan2(df['Y'],  df['-X'])\n",
    "\n",
    "    #     r = np.sqrt(df['-X']**2 + df['Y']**2 + df['Z']**2)\n",
    "\n",
    "    #     return (r, theta, phi)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def toCartesian(df):\n",
    "    #     x = self.r*np.cos(self.phi)*sin(self.theta)\n",
    "    #     y = self.r*np.sin(self.phi)*sin(self.theta)\n",
    "    #     z = self.r*np.cos(self.theta)\n",
    "    #     return Pt(x,y,z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def appendSpherical_np(xyz):\n",
    "        \n",
    "        ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\n",
    "        \n",
    "        xy = xyz[:,0]**2 + xyz[:,1]**2\n",
    "        \n",
    "        ptsnew[:,3] = np.sqrt(xy + xyz[:,2]**2)\n",
    "        \n",
    "        # for elevation angle defined from Z-axis down\n",
    "        # ptsnew[:,4] = np.arctan2(np.sqrt(xy), xyz[:,2]) \n",
    "        \n",
    "        # for elevation angle defined from XY-plane up\n",
    "        ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy)) \n",
    "        \n",
    "        ptsnew[:,5] = np.arctan2(xyz[:,1], xyz[:,0])\n",
    "        \n",
    "        return ptsnew\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize(points=None, method='open3d'):\n",
    "        \n",
    "        if method == 'open3d':\n",
    "            \n",
    "            pcd = open3d.geometry.PointCloud()\n",
    "            pcd.points = open3d.utility.Vector3dVector(points)\n",
    "            # pcd.colors = open3d.utility.Vector3dVector(data[data.columns[3]].to_numpy())\n",
    "            open3d.visualization.draw_geometries([pcd])\n",
    "            \n",
    "\n",
    "        elif method == 'pptk':\n",
    "            \n",
    "            # points = pptk.points(points)\n",
    "            v = pptk.viewer(points)\n",
    "            # v.attribute('color', data[data.columns[3]].to_numpy())\n",
    "            \n",
    "        else:\n",
    "            raiseExceptions('method should be either \"open3d\" or \"pptk\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "dataloader = DataLoader(data_type='vector', modality = 'Radar')\n",
    "data = dataloader.get_data(filename=f'{i}_.txt')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the autoencoder\n",
    "\n",
    "We are going to use the Functional API to build our convolutional autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange; font-size:0.8em\"> Vector Input </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization_Vector_Input(DataLoader):\n",
    "    \n",
    "    def __init__(self, dataset_directory='/home/artin/Documents/10dB_CR_10deg_in_2021-10-15-14-15-49' , modality='Radar'):\n",
    "\n",
    "        \n",
    "        DataLoader.__init__(self, data_type='vector' , modality=modality, dataset_directory=dataset_directory, normalize=True)\n",
    "        \n",
    "        self.data = self.get_data(filename='1_.txt')\n",
    "\n",
    "        # data2 = DataLoader(filname='2_.txt', modality=modality, dataset_directory=dataset_directory).get_data(data_type='vector' , normalize=True)\n",
    "        \n",
    "        \n",
    "        self._separate_train_valid_test(data=self.data, train_valid_test_ratio=[3 , 1 , 1])        \n",
    "        \n",
    "        self.model = self.architecture(input_shape=(self.data.shape[1],1))\n",
    "                \n",
    "    def _separate_train_valid_test(self, data , train_valid_test_ratio=[0.6, 0.2, 0.2]):\n",
    "        \n",
    "        frac = {}\n",
    "        for ix, mode in enumerate(['train' , 'valid' , 'test']):\n",
    "            frac[mode] = train_valid_test_ratio[ix]/sum(train_valid_test_ratio)\n",
    "            \n",
    "            \n",
    "        self.train_data = data.sample(frac=frac['train'], random_state=42)        \n",
    "        data.drop(self.train_data.index)\n",
    "\n",
    "        self.valid_data = data.sample(frac=frac['valid'], random_state=42)\n",
    "        data.drop(self.valid_data.index)\n",
    "        \n",
    "        self.test_data  = data.copy()\n",
    "\n",
    "    def architecture(self, input_shape: tuple):\n",
    "\n",
    "        input = layers.Input(shape=input_shape)\n",
    "\n",
    "        # Encoder\n",
    "        x = layers.Conv1D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu')(input)\n",
    "        x = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv1D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = layers.Conv1DTranspose(filters=256, strides=2, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv1DTranspose(filters=256, strides=1, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv1DTranspose(filters=1,   strides=1, kernel_size=3, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "        # Autoencoder\n",
    "        model = Model(input, x)\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, epochs=5, batch_size=32):\n",
    "        \n",
    "        self.model.fit(x=self.train_data, y=self.train_data, epochs=epochs, batch_size=batch_size, validation_data=(self.valid_data, self.valid_data))\n",
    "        \n",
    "    def predict(self, data):\n",
    "        \n",
    "        predictions = self.model.predict(data)\n",
    "        predictions = pd.DataFrame(predictions[:,:,0], columns=data.columns, index=data.index)\n",
    "        return predictions\n",
    "        \n",
    "             \n",
    "autoencoder_vectorized_input = Optimization_Vector_Input(dataset_directory='/home/artin/Documents/10dB_CR_10deg_in_2021-10-15-14-15-49' , modality='Radar')\n",
    "autoencoder_vectorized_input.fit(epochs=5, batch_size=32)\n",
    "predictions = autoencoder_vectorized_input.predict(data=autoencoder_vectorized_input.test_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange; font-size:0.8em\"> Matrix Input </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization_Matrix_Input(DataLoader):\n",
    "    \n",
    "    def __init__(self, dataset_directory='/home/artin/Documents/10dB_CR_10deg_in_2021-10-15-14-15-49' , modality='Radar'):\n",
    "\n",
    "        \n",
    "        DataLoader.__init__(self, data_type='matrix' , modality=modality, dataset_directory=dataset_directory, normalize=True)\n",
    "    \n",
    "        self.data = self.get_data(filename='1_.txt')        \n",
    "        \n",
    "        # self._separate_train_valid_test(data=self.data, train_valid_test_ratio=[3 , 1 , 1])        \n",
    "        \n",
    "        self.model = self.architecture(input_shape=(self.data.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our autoencoder using `train_data` as both our input data\n",
    "and target. Notice we are setting up the validation data using the same\n",
    "format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict on our test dataset and display the original image together with\n",
    "the prediction from our autoencoder.\n",
    "\n",
    "Notice how the predictions are pretty close to the original images, although\n",
    "not quite the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_data2 = test_data.reset_index()\n",
    "predictions2 = predictions.reset_index()\n",
    "\n",
    "sns.set()\n",
    "for y in test_data2.columns[1:]:\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='index', y=y, data=test_data2, label='test_data')\n",
    "    sns.scatterplot(x='index', y=y, data=predictions2, label='predictions')\n",
    "    plt.legend()\n",
    "    plt.xlabel('index')\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_data2-predictions2)[data.columns].plot(kind='box', figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our autoencoder works, let's retrain it using the noisy\n",
    "data as our input and the clean data as our target. We want our autoencoder to\n",
    "learn how to denoise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('autoencoder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54012e1f293dab572bba3f57e10fa4027aa2cc0dbccc79422e4471319ef5ee88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
